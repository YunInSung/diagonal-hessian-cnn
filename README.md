# diagonal-hessian-cnn

## ğŸš§ Pilot Study
*This repository contains preliminary results for a diagonal-Hessian RMSProp optimizer. Full benchmarking and ablation studies are forthcoming.*

## Abstract
This repository provides a TensorFlow implementation of a **diagonal-Hessian, RMSProp-style optimizer**, wrapped in a `MyModel` subclass that works for both **MLP** and **CNN** architectures. The CNN backbone is a compact Conv-BN-ReLU stack (3 blocks) with dropout and a 512-unit head. The optimizer uses variance scaling with bias-corrected momentum and fixed hyperparameters (**diff = 0.25**, **square = 9**, **lr = 1.0Ã—10^-6). All results are averaged over **20 deterministic seeds (0â€”19)** with **50 epochs (CIFAR-10), 60 epdochs (CIFAR-100)** per run and evaluated via paired two-tailed *t*-tests.  

### Key Results (Custom vs Adam)

#### CIFAR-10 (50 epochs)
- **val_loss**: âˆ’4.50 % (*p* = 0.01878)  
- **val_acc**: +0.59 pp (*p* = 0.00601)  
- **macro-F1**: +0.55 pp (*p* = 0.01668)  

#### CIFAR-100 (60 epochs)
- **val_loss**: âˆ’1.67 % (*p* = 0.01695)  
- **val_acc**: +0.63 pp (*p* = 0.00181)  
- **macro-F1**: +0.62 pp (*p* = 0.00103)  


### Stability
Beyond mean performance, the custom optimizer exhibits **smaller run-to-run variability** (lower standard deviations) across all metrics, indicating improved reproducibility.

  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YunInSung/diagonal-hessian-cnn/blob/main/demo.ipynb)

## ğŸ“‹ System Requirements

* **OS**: UbuntuÂ 22.04Â LTS
* **Python**:Â 3.9
* **CUDA**:Â 12.1 (nvccÂ V12.1.105)
* **cuDNN**:Â 9.9.0
* **TensorFlow**:Â 2.15.0 (XLA JIT enabled)
* **Main Libraries**

  ```text
  matplotlib==3.9.4
  numpy==1.26.4
  pandas==2.2.3
  scikit-learn==1.6.1
  tensorflow==2.15.0
  tensorflow-addons==0.22.0
  tensorflow-estimator==2.15.0
  tensorflow-io-gcs-filesystem==0.37.1
  tensorflow-probability==0.25.0
  ```

---

## ğŸ›  Installation

> **GPU vs. CPU**
> A CUDAâ€‘compatible GPU (CUDAÂ 12.1Â + cuDNNÂ 9.9) is strongly recommended for reasonable training times. If TensorFlow does not detect a GPU, the scripts automatically fall back to **CPU mode**, which can be **â‰ˆâ€¯10Ã— slower** for CIFARâ€‘10 and substantially more for larger datasets.

```bash
git clone https://github.com/YunInSung/diagonal-hessian-cnn.git
cd diagonal-hessian-cnn

# Create and activate a virtual environment
python3.9 -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

---

## Modified Custom Optimizer (RMSProp variant)

This repository builds on the original MLP implementation from [reluâ€‘basedâ€‘2ndOrderâ€‘convergence](https://github.com/YunInSung/relu-based-2ndOrder-convergence) in two ways:

1. **MLP**: directly forked and patched the original code to incorporate the changes below.  
2. **CNN**: provided a new `tf.keras.Model` subclass (`MyModel`) that embeds the same optimizer logic for convolutional architectures.

All changes applied uniformly to both variants:

- **Removed Hessianâ€‘based dynamic learning rate computed from batch loss** now using a fixed learning rate 
- **Applied standard RMSProp gradient scaling**   
- Hyperparameters adjusted to:
  - `diff = 0.25`
  - `square = 9`
  - **Learning rate** fixed at **1.0Ã—10^-6**
- **Performance**: Even on a simple MLP architecture, this configuration consistently outperforms the original optimizer across training loss and convergence speed.

## CNN variant (`MyModel`)

```text
diagonal-hessian-cnn/
â”œâ”€â”€ cnn_adam_vs_custom/
â”‚   â”œâ”€â”€ experiment_cnn_adam.py      â† Experiment: Adam vs Custom (20 runs with different seeds)
â”‚   â””â”€â”€ myModel_2opt.py             â† Custom second-order RMSProp model
â”œâ”€â”€ MLP_custom_2ndOrder_opt/        â† MLP variant (for reference)
â”‚   â”œâ”€â”€ DNN_ADAM.py
â”‚   â””â”€â”€ experiment_runner.py
â””â”€â”€ optimizer_benchmark_results/    â† Benchmark result CSVs for each dataset/optimizer combination (aggregated over 20 deterministic runs with seeds 0â€“19)
```

### Quick Start

```bash
# Adam vs Custom optimizer (default: cifar10 cifar100)
python cnn_adam_vs_custom/experiment_cnn_adam.py

# Specify dataset(s) to run one at a time (to avoid OOM)
python cnn_adam_vs_custom/experiment_cnn_adam.py --datasets cifar10
python cnn_adam_vs_custom/experiment_cnn_adam.py --datasets cifar100
```

Both scripts are self-contained; simply run them to reproduce the experiments.
If you encounter GPU out-of-memory errors, run one dataset at a time using the `--datasets` option or lower the batch size.

### Whatâ€™s inside `myModel_2opt.py`?

* **Nested `GradientTape`** to capture the gradient *and* the diagonal Hessian in one pass.
* **RMSProp-style variance scaling** combined with bias-corrected first-order momentum.
* A custom `train_step()` that applies a second-order RMSProp update:

  $$
  \theta \leftarrow \theta - \text{lr} \; \frac{\hat m}{\sqrt{\hat v} + \varepsilon}
  $$

The two `myModel_2opt.py` files are identical apart from their default hyper-parameters.

> **Tip**
> Diagonalâ€‘Hessian extraction can be memoryâ€‘hungry. If you run into GPU limits, reduce the batch size or enable `jit_compile=True` for XLA acceleration.

## Adam vs Custom Performance Comparison Results

> **Note**: All results are based on 50 epochs of training.

### 1. Experimental Setup
| Item | Details |
|------|---------|
| **Datasets** | CIFAR-10, CIFAR-100 (Keras default train / test splits) |
| **Model** | 3 Ã— {Conv-BN-ReLU-MaxPool-Dropout} â†’ Dense 512 â†’ Softmax |
| **Baselines** | **Adam** (default LR = 0.001, Î²â‚ = 0.9, Î²â‚‚ = 0.999) |
| **Custom** | `MyModel` (diagonal-Hessian second-order method) â€“ identical network |
| **Runs** | 20 deterministic seeds (0â€“19), same list for both optimizers |
| **Metrics** | `val_loss`, `val_acc`, macro `f1`, `train_time` |
| **Statistics** | Paired two-tailed *t*-test (Î± = 0.05); effect size implicit via % change |

---

### 2. Aggregate Results

#### 2.1 CIFAR-10

| Metric | Adam Mean Â± SD | Custom Mean Â± SD | Î” (Customâ€“Adam) | % Change | *t* | *p*-value | Sig. |
|--------|----------------|------------------|-----------------|----------|-----|-----------|------|
| **val_loss** | 0.4332 Â± 0.0294 | 0.4137 Â± 0.0147 | âˆ’0.0195 | âˆ’4.50 % | -2.569 | 0.01878 | * |
| **val_acc** | 0.8622 Â± 0.0072 | 0.8681 Â± 0.0042 | +0.0059 | +0.69 % | +3.091 | 0.00601 | * |
| **f1** | 0.8614 Â± 0.0078 | 0.8669 Â± 0.0048 | +0.0055 | +0.64 % | +2.625 | 0.01668 | * |
| **train_time** | 248.8213 Â± 2.2744 s | 311.3269 Â± 4.2343 s | +62.5056 s | +25.12 % | +76.353 | 0.00000 | ** |

**Key points**

* Custom lowers loss (**âˆ’4.5 %**), *p* = 0.01878.
* Accuracy improves by **+0.59 pp**, *p* = 0.00601.
* F1 improves by **+0.55 pp**, *p* = 0.01668.
* Training-time overhead **+25.12 %**.
---

#### 2.2 CIFAR-100

| Metric | Adam Mean Â± SD | Custom Mean Â± SD | Î” (Customâ€“Adam) | % Change | *t* | *p*-value | Sig. |
|--------|----------------|------------------|-----------------|----------|-----|-----------|------|
| **val_loss** | 1.4717 Â± 0.0221 | 1.4472 Â± 0.0321 | âˆ’0.0245 | âˆ’1.67 % | -2.617 | 0.01695 | * |
| **val_acc** | 0.5910 Â± 0.0045 | 0.5973 Â± 0.0061 | +0.0063 | +1.07 % | +3.624 | 0.00181 | * |
| **f1** | 0.5872 Â± 0.0042 | 0.5934 Â± 0.0061 | +0.0062 | +1.06 % | +3.870 | 0.00103 | * |
| **train_time** | 315.6871 Â± 7.1132 s | 381.5336 Â± 9.6949 s | +65.8465 s | +20.86 % | +65.231 | 0.00000 | ** |

**Key points**

* Custom reduces loss (**-1.67 %**), *p* = 0.01695.
* Accuracy improves by **+0.63 pp**, *p* = 0.00181.
* F1 improves by **+0.62 pp**, *p* = 0.00103.
* Training-time overhead **+20.86 %**.

---

#### Significance Flags
* `*` *p* < 0.05â€ƒ `**` *p* < 0.001

### 3. Interpretation

| Aspect | Summary |
|--------|---------|
| **Performance** | **CIFAR-10:** significant gains in loss (âˆ’4.5%, *p* = 0.0188), acc (+0.59 pp, *p* = 0.0060), and F1 (+0.55 pp, *p* = 0.0167). **CIFAR-100:** consistent, significant improvements across loss (âˆ’1.67%, *p* = 0.0170), acc (+0.63 pp, *p* = 0.0018), and F1 (+0.62 pp, *p* = 0.0010). |
| **Stability** | **CIFAR-10:** Custom shows slightly lower run-to-run variance (smaller SDs). **CIFAR-100:** SDs are comparable or slightly higher for Custom. |
| **Compute Cost** | Diagonal-Hessian updates add â‰ˆ**+25.1%** (CIFAR-10) and â‰ˆ**+20.9%** (CIFAR-100) training time. |

